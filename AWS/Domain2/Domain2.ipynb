{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;color:orange\">Domain2-Exploratory Data Analysis</h1>\n",
    "<p style=\"text-align:right;\">&copy; FebaTech</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"back\"></div>\n",
    "\n",
    "## Table of Content\n",
    "<a href=\"#GLUE\">Glue</a>\n",
    "- Creating a crawler\n",
    "- Using ETL to Transform .csv to .json\n",
    "<br>\n",
    "\n",
    "<a href=\"#Athena\">Athena</a><br>\n",
    "- Athena for simple SQL queries\n",
    "<br> \n",
    "\n",
    "<a href=\"#QuickSight\">QuickSight</a><br>\n",
    "<a href=\"#EMR\">EMR</a>\n",
    "<br>\n",
    "<a href=\"#SageMaker\">Sagemaker</a><br>\n",
    "- Preprocess and visualization with Sagemaker notebook\n",
    "<br>\n",
    "\n",
    "<a href=\"#Feature Engineering\">Feature Engineering</a>\n",
    "- Imputing Missing Data\n",
    "- Handling Unbalanced Data\n",
    "- Handling Outliers\n",
    "- Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets upload our dataset to s3 bucket\n",
    "<b style=\"color:red\">NOTE</b> Bucket name should be Unique(Globally) so you cannot create a bucket name same a mine\n",
    "\n",
    "Do remember the bucket name and use it instead of what I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"ETL\" height=500px width=900px controls>\n",
       "        <source src=\"assets/S3.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"ETL\" height=500px width=900px controls>\n",
    "        <source src=\"assets/S3.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLUE\n",
    "\n",
    "<img src=\"assets/AWS-Glue@4x.png\" alt=\"Glue\"></img>\n",
    "\n",
    "AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g., table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL.\n",
    "\n",
    "- Step 1: Build your Data Catalog(Metadata repository for all your tables) \n",
    "Glue crawler crawls your data in an s3 bucket and builds tables automatically making the bucket ready for querying with Athena or Redshift Spectrum\n",
    "i.e., Crawlers go through your data to infer schemas and partitions \n",
    "Works JSON, Parquet, CSV, relational store. Works for S3, Redshift, RDS\n",
    "- Step 2: Generate and Edit Transformations\n",
    "AWS Glue will generate ETL code in Scala or Python to extract data from the source, transform the data to match the target schema and load it into the target. Runs on serverless spark platform.\n",
    "Some Examples of Transformations\n",
    "DropFields that are unwanted, DropNullFields, Filter, Join, Mapping the data structures of columns in the table,.. these are called bundled transformations.FindMatches to detect duplicates even the unique identifier is not the same, etc It is a ML transformation. \n",
    "- Step 3: Schedule and Run Your Jobs\n",
    "AWS Glue makes it easy to schedule recurring ETL jobs, chain multiple jobs together, or invoke tasks on-demand from other services like AWS Lambda. It automatically scales the resources and retries if the job fails.\n",
    "\n",
    "**Note** Crawlers will extract partitions based on how you organised your S3 like a folder for year inside folders for months with folders for date, ...<br>\n",
    "Crawler will create partition with w.r.t the divisions\n",
    "\n",
    "<a href=\"#back\">Table of content</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial we will crawl the s3 bucket from above that contains .csv file to get the schema of it**\n",
    "<img src=\"assets/Crawler_Flow.png\" alt=\"FLow\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"Glue_Crawler\" height=500px width=900px controls>\n",
       "        <source src=\"assets/Glue_Crawler.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"Glue_Crawler\" height=500px width=900px controls>\n",
    "        <source src=\"assets/Glue_Crawler.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial we will transform the csv file to json file with ETL and save it to S3**\n",
    "<img src=\"assets/ETL_Flow.png\" alt=\"Flow\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"ETL\" height=500px width=900px controls>\n",
       "        <source src=\"assets/ETL.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"ETL\" height=500px width=900px controls>\n",
    "        <source src=\"assets/ETL.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athena\n",
    "<img height=200px width=200px src=\"assets/Amazon-Athena@4x.png\" alt=\"Athena\"></img>\n",
    "\n",
    "- Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage. It supports many formats like csv, Json etc..\n",
    "\n",
    "- Athena is out-of-the-box integrated with AWS Glue Data Catalog, allowing you to create a unified metadata repository across various services, crawl data sources to discover schemas, and populate your Catalog with new and modified table and partition definitions, and maintain schema versioning.\n",
    "\n",
    "## Security\n",
    "\n",
    "- Access control with IAM, S3 bucket policies etc\n",
    "- Encrypt results in S3 staging directory\n",
    "- Server-side encryption with S3-managed/KMS key\n",
    "- Client-side encryption with KMS key\n",
    "- Cross-account access in S3 bucket policy\n",
    "- Transport Layer Security(TLS) encrypts in transit (between Athena and S3)\n",
    "\n",
    "<a href=\"#back\">Table of content</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial we will see how to use Athena to query from the tables we created with Glue-Crawler**\n",
    "<img src=\"assets/Athena_Flow.png\" alt=\"Flow\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"Athena\" height=500px width=900px controls>\n",
       "        <source src=\"assets/Athena.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"Athena\" height=500px width=900px controls>\n",
    "        <source src=\"assets/Athena.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuickSight\n",
    "\n",
    "<img src=\"assets/Amazon-Quicksight@4x.png\" alt=\"QuickSight\"></img>\n",
    "- Amazon QuickSight is a fast, serverless, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization.\n",
    "- QuickSight lets you easily create and publish interactive dashboards that include ML Insights. Panels can then be accessed from any device and embedded into your applications, portals, and websites.\n",
    "- Input can be provided from various sources like S3, Redshift, Athena, and It allows limited ETL for data preparation.With SPICE(super-fast, parallel, in-memory, calculation engine), QuickSightâ€™s in-memory calculation engine, you achieve blazing fast performance at scale. It automatically replicates data for high availability allowing thousands of users to perform fast simultaneously, interactive analysis while shielding your underlying data infrastructure, saving you time and resources.\n",
    "- In a nutshell, Quick sight services provide tools for Analyze / visualize data, Stories(  Convey key points, thought process, the evolution of an analysis)\n",
    "It has ML Insights, which helps in anomaly detection, Forecasting, and Auto-narratives of your data.\n",
    " \n",
    "### QuickSight Visual Types :\n",
    "- Bar Charts: For comparison and distribution (histograms) \n",
    "- Line graphs:  For changes over time \n",
    "- Scatter plots, heat maps: For correlation \n",
    "- Pie graphs, treemaps: For aggregation \n",
    "- Pivot tables: For tabular data\n",
    "- Auto Graphs: Automatically detects which graphs best suits for your data\n",
    "\n",
    "### Security\n",
    "- Multi-factor authentication\n",
    "- VPC and private VPC connectivity\n",
    "- Add QuickSightâ€™s IP address range to your database security groups\n",
    "- Row-level security\n",
    "- Elastic Network Interface, AWS Direct Connect\n",
    "- Users defined via IAM, or email signup\n",
    "\n",
    "<a href=\"#back\">Table of content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR\n",
    "\n",
    "<img src=\"assets/Amazon-EMR@4x.png\" alt=\"EMR\"></img>\n",
    "\n",
    "- Amazon EMR is a managed Hadoop framework on EC2 instances, which is a cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\n",
    "\n",
    "- An EMR provides a way of distributing the load of computation of data across a cluster of computers. A Cluster in an EMR is a collection of EC2 instances called nodes.<br>\n",
    "**Master Node: Manages the cluster<br>\n",
    "Core Nodes: Hosts Hadoop file system data and runs tasks<br>\n",
    "Task Nodes: Runs Tasks but does not host data**<br>\n",
    "<br>\n",
    "- Transient vs Long-Running Clusters\n",
    "- Can spin up task nodes using Spot instances for temporary capacity\n",
    "- Can use reserved instances on long-running clusters to save money\n",
    "- Connect directly to master to run jobs\n",
    "- Submit ordered steps via the console\n",
    "\n",
    "**_How many core nodes/Task nodes you need depends on load/data_**\n",
    "\n",
    "### Integrate EMR with other services\n",
    "- Amazon EC2 for the instances that comprise the nodes in the cluster\n",
    "- Amazon VPC to configure the virtual network in which you launch your instances\n",
    "- Amazon S3 to store input and output data\n",
    "- Amazon CloudWatch to monitor cluster performance and configure alarms\n",
    "- AWS IAM to configure permissions\n",
    "- AWS CloudTrail to audit requests made to the service\n",
    "- AWS Data Pipeline to schedule and start your clusters\n",
    "\n",
    "\n",
    "\n",
    "### What is Hadoop:\n",
    "In a nutshell Hadoop is a framework that allows you to first store Big Data in a distributed environment, so that, you can process it parallely.\n",
    "- Hadoop consists of 3 layers. \n",
    "- **HDFS: The storage layer**\n",
    "- **YARN: For job scheduling and cluster resource management.**\n",
    "- **MAP REDUCE: MapReduce is a parallel programming model for writing distributed applications devised at Google for efficient processing of large amounts of data (multi-terabyte data-sets), on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.**\n",
    "\n",
    "- But Apache spark Sits on top of YARN replacing MAP REDUCE nowadays as it is faster than Map Reduce \n",
    "Spark consists of several modules Spark Streaming, Spark ML library, Spark SQL, etc\n",
    "Spark ML library consists of ML algorithms that are rewritten to support distributed and scalable.\n",
    "\n",
    "- Zepplin: This is a notebook instance where you can run spark code interactively.\n",
    "\n",
    "<a href=\"#back\">Table of content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker\n",
    "<img src=\"assets/Amazon-SageMaker_light-bg@4x.png\" alt=\"SageMaker\"></img>\n",
    "\n",
    "- Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly.\n",
    "- You can create Notebook Instances on EC2 from the console. The data flow is from s3 and one can integrate Spark with Sagemaker. It has support for many Ml/Dl APIs and Libraries. Apart from this Sagemaker has its own set of ML algoâ€™s you can learn more about Sagemaker in Domain3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial we will preprocess and visualize our house prediction dataset with pandas, seaborn with sagemaker notebook. The notebook which we are going to use is provided, housing-visual-preprocess-demo.ipynb .**\n",
    "<br>An **Amazon SageMaker notebook** instance is a machine learning (ML) compute instance running the Jupyter Notebook App. SageMaker manages creating the instance and related resources. Use Jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to SageMaker hosting, and test or validate your models.\n",
    "<img src=\"assets/sagemaker_flow.png\" alt=\"Flow\"></img>\n",
    "<br>\n",
    "**Boto** is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS services.\n",
    "\n",
    "<a href=\"#back\">Table of content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"ETL\" height=500px width=900px controls>\n",
       "        <source src=\"assets/sagemaker.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"ETL\" height=500px width=900px controls>\n",
    "        <source src=\"assets/sagemaker.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Feature Engineering\"></div>\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "### Imputing Missing Data\n",
    "**Get more data**\n",
    "- It is the best way to not introducing bias or any data blindly which throws away relationships b/w data.\n",
    "<br>\n",
    "\n",
    "**Replace with Mean/Median of the column.**\n",
    "- These may introduce bias and doesnâ€™t care about relation b/w columns \n",
    "- Mean is not a good choice in presence of outliers hence the median can be chosen.\n",
    "<br>\n",
    "\n",
    "**Drop the rows**\n",
    "- It is a good method if many rows are not missing data. But take care as it may introduce bias\n",
    "<br>\n",
    "\n",
    "**Use Unsupervised algorithms to fill that data.**\n",
    "- This is a nice method but it is computationally costly.\n",
    "- Algorithms like K-Means, MICE are used\n",
    "\n",
    "### Handling Unbalanced Data:\n",
    "**Defining weighted Loss function**\n",
    "- As the loss function for any machine learning model gives equal importance to all classes, but as the data set is unbalanced, we can redefine our loss function to provide more weight for the minority class.\n",
    "- The weights are defined as \n",
    "- No of examples of majority class/total examples for the minority class\n",
    "- No of examples of minority class/total examples for the majority class\n",
    "<br>\n",
    "\n",
    "**Over Sampling**\n",
    "- Duplicate samples from the minority class\n",
    "<br>\n",
    "\n",
    "**Under Sampling**\n",
    "- Remove samples of the majority class. This is not the best voice unless you want to avoid big data.\n",
    "<br>\n",
    "\n",
    "**Adjusting Threshold**\n",
    "- Increasing the threshold to identify correctly\n",
    "- Example: if the probability of being positive is 0.6 donâ€™t assign it as a positive case instead up the threshold to more percentage where your metrics seem acceptable.\n",
    "<br>\n",
    "\n",
    "**SMOTE(Synthetic Minority Over-sampling Technique)**\n",
    "- Artificially generate new samples of the minority class using nearest neighbors.\n",
    "\n",
    "\n",
    "### Handling Outliers:\n",
    "**Varience**\n",
    "- With the help of variance, we can find out the outliers. As most of the real word problems matches with Normal distribution. Outliers are considered as points that lie out of +- n*times standard deviation. N is decided w.r.t business problem\n",
    "<br>\n",
    "\n",
    "**Unsupervised Algorithms**\n",
    "- We can take help of some Unsupervised ML algorithms to take care of this eg:- enhanced K-Means\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "**Binning**\n",
    "- Bucket observations together based on ranges of values. It is useful when there is uncertainty in Measurements Transforming\n",
    "- Applying some function to a feature to make it better suited for training. For example, if there is data that has some exponential property instead of taking the data directly it will be ideal if log(data) is taken which makes the output to be linear\n",
    "<br>\n",
    "\n",
    "**Encoding**\n",
    "- Suppose in a data there are a set of different colors as a column, \n",
    "- Instead of using 1,2,3 To represent colors, it will be meaningful to encoded them as [0 0 1], [0 1 0],[1, 0, 0]. This is called one-hot encoding. If we donâ€™t, our model can understand that increasing property (1,2,3) contributes some weight in deciding classes.\n",
    "<br>\n",
    "\n",
    "**Scaling / Normalization**\n",
    "- Decreases Bias, Otherwise features with larger magnitudes will have more weight than they should\n",
    "<br>\n",
    "\n",
    "**Shuffling**\n",
    "- It is a very important preprocess to shuffle data so that our model can learn different features to identify different classes rather than learning features to distinguish one class and then others, which makes the model forget.\n",
    "<br>\n",
    "\n",
    "**Column manipulations**\n",
    "- Sometimes the data seems to be exponential; hence transforming the entire column with a logarithmic function helps linear regression to catch up easily\n",
    "- Sometimes we need the area to design our model, but length and width are provided as two columns, so replacing them with one column will reduce dimension.\n",
    "\n",
    "<a href=\"#back\">Table of content</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
