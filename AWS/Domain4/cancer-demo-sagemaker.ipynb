{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and deploying our Keras CNN on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified our Transfer learning Keras model from the basic section to cancerDemo.py script that you must upload into this Notebook's directory. It's the same code as the previous exercise, but with a little bit of extra stuff to allow hyperparameters to be passed in as arguments from SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to disk\n",
    "Downloading data from github to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "file_url = \"https://github.com/satoru2001/Cancer_Dataset/raw/master/dataset.zip\"\n",
    "    \n",
    "r = requests.get(file_url, stream = True)\n",
    "\n",
    "#Downloading data\n",
    "with open(\"dataset.zip\", \"wb\") as file:  \n",
    "  for block in r.iter_content(chunk_size = 1024): \n",
    "    if block:\n",
    "      file.write(block)\n",
    "\n",
    "#Extracting zip file\n",
    "import zipfile\n",
    "loc_ref = \"dataset.zip\"\n",
    "zip_ref = zipfile.ZipFile(loc_ref)\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to S3\n",
    "As EC2 instance need data to be fed in through S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "training_folder = os.path.join(cwd,\"tmp/Train\")\n",
    "prefix = 'cancer'\n",
    "\n",
    "training_input_path   = sess.upload_data(training_folder, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our instance for single epoch to test every thing is correct before proceeding further\n",
    "\n",
    "We'll test out running a single epoch, just to make sure the script works before we start spending money on other instances. Here as it is a demo you can skip train on a GPU cell as it costs money.\n",
    "\n",
    "As we are training for 1 epoch the accuracy will be less\n",
    "\n",
    "If you want to train for more epochs change the epoch parameter in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize cancerDemo.py #To display our cancerDemo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "#Train a custom TensorFlow model in local instance.\n",
    "tf_estimator = TensorFlow(entry_point='cancerDemo.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='2.0', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 1}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a GPU instance with 10 epochs\n",
    "\n",
    "Now that we're sure it works, you can use gpu's to train<br>\n",
    "**Note** It costs money so I commented if any one want to try you can uncomment and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a custom TensorFlow model in P3 instance\n",
    "#tf_estimator = TensorFlow(entry_point='mnist-train-cnn.py', \n",
    "#                           role=role,\n",
    "#                           train_instance_count=1, \n",
    "#                           train_instance_type='ml.p3.2xlarge',\n",
    "#                           framework_version='2.00', \n",
    "#                           py_version='py3',\n",
    "#                           script_mode=True,\n",
    "#                           hyperparameters={'epochs': 10}\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_estimator.fit({'training': training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "### For this we have plenty of options\n",
    "1. Choose GPU instance as above but it costs much\n",
    "1. choose High end CPU instance + Elastic Inference accelerator which costs significantly less with high performence\n",
    "1. Use our local instance as this is not a complex algo\n",
    "\n",
    "we will go with 3 as it doesn't cost any money.\n",
    "\n",
    "If you want to try second one use\n",
    "```\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.c5.large',        \n",
    "                         accelerator_type='ml.eia1.medium',  \n",
    "                         endpoint_name=tf_endpoint_name) \n",
    "```\n",
    "instead of below code for tf_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-mnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "#Creating an endpoint to work deploy model\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                         instance_type='local',  \n",
    "                         endpoint_name=tf_endpoint_name)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "num_samples = 5\n",
    "train_dir = os.path.join(cwd,\"tmp/Train\")\n",
    "IDC = os.path.join(train_dir,'IDC')\n",
    "NONIDC = os.path.join(train_dir,'NONIDC')\n",
    "prediction_array = []\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)#Plots\n",
    "    img = load_img(IDC + \"/\" +os.listdir(IDC)[i], target_size=(50, 50))#fetch image from directory\n",
    "    prediction_array.append(img_to_array(img))#Store image array need to be sen for predictions\n",
    "    plt.imshow(img)#Display image\n",
    "    \n",
    "prediction_array = np.array(prediction_array)\n",
    "prediction_array.shape\n",
    "prediction = tf_predictor.predict(prediction_array)['predictions']#Sending array for predictions\n",
    "prediction = np.array(prediction)\n",
    "for i in prediction:\n",
    "    print(\"NONIDC\") if i>0.5 else print(\"IDC\")\n",
    "print(\"Here we passed all IDC i.e Positive as we train our model only once our model cannot predict every thing correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the deployment endpoint\n",
    "If you are using other than local instance for deployment\n",
    "\n",
    "you can delete endpoint from console also in Sagemaker\n",
    "\n",
    "Under Inference, choose Endpoints.\n",
    "\n",
    "Choose the endpoint that you created in the example, then choose Actions | Delete.\n",
    "\n",
    "Choose Delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to shut down your notebook instance too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
